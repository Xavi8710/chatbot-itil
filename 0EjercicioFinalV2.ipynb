{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47350d72",
   "metadata": {},
   "source": [
    "# Khipus.ai Hackathon 2025 - Universidad NUR\n",
    "## Practico Final : ChatGPT - ITIL\n",
    "### Team Itil           : Andres Moron - Javier Castellon Galvis - Huascar Rivero Lara\n",
    "\n",
    "<span>© Copyright Notice 2025, Khipus.ai - All Rights Reserved.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71143251",
   "metadata": {},
   "source": [
    "### Paso 1. Instalar dependencias necesarias\n",
    "\n",
    "    ChromaDB                Es una base de datos vectorial open-source  \n",
    "    sentence-transformers   Proporciona modelos preentrenados para generar embeddings de texto  \n",
    "    PyPDF                   Es una biblioteca para manipular archivos PDF en Python     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e705d7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [], 'embeddings': None, 'documents': [], 'uris': None, 'included': ['metadatas', 'documents'], 'data': None, 'metadatas': []}\n",
      "\n",
      "[INFO]: El número de documentos en la colección 'itil_docs' es: 0\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "# # Este script inicializa un cliente ChromaDB, recupera una colección e imprime todos los documentos y sus metadatos, junto con el recuento de documentos en la colección.\n",
    "\n",
    "from chromadb.utils import embedding_functions      # Para usar funciones de incrustación\n",
    "from chromadb.config import Settings                # Para configurar el cliente ChromaDB\n",
    "import chromadb                                     # Importar el cliente ChromaDB\n",
    "\n",
    "# Inicializar el cliente ChromaDB con un directorio persistente\n",
    "chroma_client = chromadb.Client(Settings(persist_directory=\"./chroma_db\"))  # Configurar el directorio persistente\n",
    "collection = chroma_client.get_or_create_collection(\"itil_docs\")            # Recuperar o crear la colección llamada \"itil_docs\"\n",
    "\n",
    "# Para ver todos los documentos y metadatos en la colección\n",
    "all_data = collection.get()                         # Recuperar todos los documentos y metadatos de la colección\n",
    "print(all_data)                                     # Imprimir todos los documentos y metadatos\n",
    "\n",
    "#  Para contar el número de documentos en la colección\n",
    "count = collection.count()                          # Contar el número de documentos en la colección    \n",
    "print(\"\\n[INFO]: El número de documentos en la colección 'itil_docs' es:\", count)  # Imprimir el número de documentos en la colección   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5383dc",
   "metadata": {},
   "source": [
    "### Paso 2 : Chunking - Embeddings - Carga Vector Database (consistent)\n",
    "\n",
    "    Chunking - Carga y divide documentos \n",
    "    Crea embeddings con Ollama \n",
    "    Carga en ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d6a155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos cargados: 30. Fragmentos creados: 178.\n",
      "\n",
      "[INFO]: Creando embeddings con Ollama. Esto puede tardar mucho en CPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yayo Rivero\\AppData\\Local\\Temp\\ipykernel_7284\\117736389.py:22: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"llama2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ÉXITO]: ChromaDB ha sido poblada con los embeddings.\n"
     ]
    }
   ],
   "source": [
    "import os                                                                       # Importar las bibliotecas necesarias de LangChain y ChromaDB\n",
    "from langchain_community.document_loaders import PyPDFLoader                    # Para cargar documentos PDF\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter             # Para dividir documentos en fragmentos\n",
    "from langchain_community.embeddings import OllamaEmbeddings                     # Para crear embeddings con Ollama\n",
    "from langchain_community.llms import Ollama                                     # Para usar el modelo LLM de Ollama    \n",
    "from langchain_community.vectorstores import Chroma                             # Para usar ChromaDB como base de datos vectorial    \n",
    "from langchain.chains import RetrievalQA                                        # Para crear una cadena de preguntas y respuestas (RAG)    \n",
    "\n",
    "# Chunking - Carga y divide documentos ---\n",
    "# La ruta del PDF se mantiene igual\n",
    "pdf_path = \"./docs/ITIL4Intro.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)          # Carga el PDF usando PyPDFLoader\n",
    "documents = loader.load()               # Carga los documentos del PDF\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)   # Configura el divisor de texto para dividir los documentos en fragmentos\n",
    "docs = text_splitter.split_documents(documents)                                     # Divide los documentos en fragmentos\n",
    "print(f\"Documentos cargados: {len(documents)}. Fragmentos creados: {len(docs)}.\")\n",
    "\n",
    "# Crea embeddings con Ollama ---\n",
    "# Usa el modelo de embeddings de Ollama, que por defecto usará 'llama2'\n",
    "print(\"\\n[INFO]: Creando embeddings con Ollama. Esto puede tardar mucho en CPU...\")\n",
    "embeddings = OllamaEmbeddings(model=\"llama2\")                                       # Configura el modelo de embeddings de Ollama\n",
    "\n",
    "# Carga en ChromaDB ---\n",
    "# Crea o se conecta a la base de datos vectorial persistente\n",
    "vectordb = Chroma.from_documents(                                                   # Crea la base de datos vectorial a partir de los documentos\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    collection_name=\"itil_docs\"\n",
    ")\n",
    "print(\"[ÉXITO]: ChromaDB ha sido poblada con los embeddings.\")\n",
    "\n",
    "# Duracion 35 minutos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a54b17",
   "metadata": {},
   "source": [
    "### Paso 3 : Configuración - Retriever - RAG\n",
    "\n",
    "    Configura el LLM local con Ollama \n",
    "    Crea un retriever para buscar en la base de datos \n",
    "    Cadena de preguntas y respuestas\n",
    "    Invoca la cadena RAG con la consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa59abc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Conectando con el LLM de Ollama...\n",
      "[ÉXITO]: LLM de Ollama listo.\n",
      "\n",
      "[INFO]: Preparando para hacer una consulta a la IA...\n",
      "\n",
      "[INFO]: Haciendo la consulta: '¿Qué es ITIL?'\n",
      "\n",
      "Respuesta de la IA:\n",
      "ITIL stands for Information Technology Infrastructure Library. It is a framework of guidelines and best practices for IT service management, which helps organizations to manage their IT infrastructure and deliver high-quality services to their customers. The ITIL framework covers four dimensions of service management - Service Strategy, Service Design, Service Transition, and Service Operation - and provides a structure for organizing and managing IT services in a way that meets the needs of the organization and its customers.\n"
     ]
    }
   ],
   "source": [
    "# Configura el LLM local con Ollama ---\n",
    "# Usa el modelo de Ollama 'llama2' para la generación de texto\n",
    "print(\"[INFO]: Conectando con el LLM de Ollama...\")\n",
    "llm = Ollama(model=\"llama2\")                # Configura el modelo LLM de Ollama\n",
    "print(\"[ÉXITO]: LLM de Ollama listo.\")\n",
    "\n",
    "# Crea el pipeline RAG y haz una pregunta ---\n",
    "# Crea un retriever para buscar en la base de datos\n",
    "retriever = vectordb.as_retriever()         # Configura el retriever para buscar en la base de datos vectorial\n",
    "\n",
    "# Crea la cadena de preguntas y respuestas (RAG) ---\n",
    "# Crea la cadena RAG con el LLM y el retriever\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)    # Configura la cadena de preguntas y respuestas\n",
    "\n",
    "# Haz una consulta ---  \n",
    "print(\"\\n[INFO]: Preparando para hacer una consulta a la IA...\")\n",
    "# Haz una consulta\n",
    "query = \"¿Qué es ITIL?\"\n",
    "print(f\"\\n[INFO]: Haciendo la consulta: '{query}'\")\n",
    "response = qa_chain.invoke({\"query\": query})        # Invoca la cadena RAG con la consulta\n",
    "print(\"\\nRespuesta de la IA:\")\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f35c183",
   "metadata": {},
   "source": [
    "### Paso Extra : Consulta a la DB Vectorial desde 0\n",
    "\n",
    "    Configura el modelo de embeddings de Ollama\n",
    "    Carga la base de datos vectorial desde el directorio persistente\n",
    "    Inicializar el modelo LLM de Ollama\n",
    "    Crear el retriever y la cadena QA\n",
    "    Realizar una pregunta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb692ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Inicializando embeddings de Ollama...\n",
      "[INFO]: Cargando base de datos Chroma desde disco...\n",
      "[ÉXITO]: Base de datos Chroma cargada.\n",
      "[INFO]: Conectando con el LLM de Ollama...\n",
      "[ÉXITO]: LLM de Ollama listo.\n",
      "\n",
      "[Consulta]: ¿Qué sabes del motor de un auto Corolla Cross? Respóndeme en español.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yayo Rivero\\AppData\\Local\\Temp\\ipykernel_3280\\332879851.py:16: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=OLLAMA_MODEL)\n",
      "C:\\Users\\Yayo Rivero\\AppData\\Local\\Temp\\ipykernel_3280\\332879851.py:20: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(\n",
      "C:\\Users\\Yayo Rivero\\AppData\\Local\\Temp\\ipykernel_3280\\332879851.py:29: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=OLLAMA_MODEL)\n",
      "C:\\Users\\Yayo Rivero\\AppData\\Local\\Temp\\ipykernel_3280\\332879851.py:43: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  respuesta = qa_chain.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Respuesta]:\n",
      "I don't know anything about the Toyota Corolla Cross engine as it is not related to the context provided in the passage. The passage discusses ITIL, a framework for continuous service improvement, and its components such as Plan-Do-Check-Act and the importance of creating value for customers. It does not mention anything about cars or their engines.\n"
     ]
    }
   ],
   "source": [
    "# --- IMPORTS NECESARIOS ---\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# --- CONFIGURACIÓN ---\n",
    "CHROMA_PATH = \"./chroma_db\"\n",
    "COLLECTION_NAME = \"itil_docs\"\n",
    "OLLAMA_MODEL = \"llama2\"  # Puedes usar otro modelo si tienes: \"mistral\", \"phi\", etc.\n",
    "\n",
    "# --- PASO 1: Crear los embeddings ---\n",
    "print(\"[INFO]: Inicializando embeddings de Ollama...\")\n",
    "embeddings = OllamaEmbeddings(model=OLLAMA_MODEL)       # Configura el modelo de embeddings de Ollama\n",
    "\n",
    "# --- PASO 2: Cargar la base de datos vectorial existente ---\n",
    "print(\"[INFO]: Cargando base de datos Chroma desde disco...\")\n",
    "vectordb = Chroma(                                      # Carga la base de datos vectorial desde el directorio persistente\n",
    "    persist_directory=CHROMA_PATH,\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=COLLECTION_NAME\n",
    ")\n",
    "print(\"[ÉXITO]: Base de datos Chroma cargada.\")\n",
    "\n",
    "# --- PASO 3: Inicializar el modelo LLM de Ollama ---\n",
    "print(\"[INFO]: Conectando con el LLM de Ollama...\")\n",
    "llm = Ollama(model=OLLAMA_MODEL)                        # Configura el modelo LLM de Ollama\n",
    "print(\"[ÉXITO]: LLM de Ollama listo.\")\n",
    "\n",
    "# --- PASO 4: Crear el retriever y la cadena QA ---\n",
    "retriever = vectordb.as_retriever()                     # Configura el retriever para buscar en la base de datos vectorial\n",
    "qa_chain = RetrievalQA.from_chain_type(                 # Configura la cadena de preguntas y respuestas\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "# --- PASO 5: Realizar una pregunta ---\n",
    "query = \"¿Qué sabes del motor de un auto Corolla Cross? Respóndeme en español.\"\n",
    "print(f\"\\n[Consulta]: {query}\")\n",
    "respuesta = qa_chain.run(query)\n",
    "print(f\"\\n[Respuesta]:\\n{respuesta}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
